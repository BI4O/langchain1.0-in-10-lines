from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain.agents.middleware import before_model, after_model, AgentState
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import InMemorySaver

load_dotenv()

class CustomState(AgentState):
    model_call_count: int
    user_id: str

# Store ç‰ˆæœ¬ï¼šè·¨çº¿ç¨‹æŒä¹…åŒ–
@before_model(state_schema=CustomState, can_jump_to=["end"])
def count_with_store(state: CustomState, runtime):
    count = state.get("model_call_count", 0)
    print(f"=== Storeç‰ˆæœ¬ - Before Model Call ===")
    print(f"model call count: {count}")
    print(f"user_id: {state.get('user_id', 'unknown')}")

    if count >= 2:
        print("ğŸ›‘ Storeç‰ˆæœ¬ï¼šè°ƒç”¨æ¬¡æ•°è¶…è¿‡é™åˆ¶")
        return {"jump_to": "end"}
    return None

@after_model(state_schema=CustomState)
def increment_with_store(state: CustomState, runtime):
    old_count = state.get("model_call_count", 0)
    new_count = old_count + 1
    print(f"=== Storeç‰ˆæœ¬ - After Model Response ===")
    print(f"count updated from {old_count} to {new_count}")
    return {"model_call_count": new_count}

# Checkpointer ç‰ˆæœ¬ï¼šçº¿ç¨‹å†…æŒä¹…åŒ–
@before_model(state_schema=CustomState, can_jump_to=["end"])
def count_with_checkpointer(state: CustomState, runtime):
    count = state.get("model_call_count", 0)
    print(f"=== Checkpointerç‰ˆæœ¬ - Before Model Call ===")
    print(f"model call count: {count}")
    print(f"user_id: {state.get('user_id', 'unknown')}")

    if count >= 2:
        print("ğŸ›‘ Checkpointerç‰ˆæœ¬ï¼šè°ƒç”¨æ¬¡æ•°è¶…è¿‡é™åˆ¶")
        return {"jump_to": "end"}
    return None

@after_model(state_schema=CustomState)
def increment_with_checkpointer(state: CustomState, runtime):
    old_count = state.get("model_call_count", 0)
    new_count = old_count + 1
    print(f"=== Checkpointerç‰ˆæœ¬ - After Model Response ===")
    print(f"count updated from {old_count} to {new_count}")
    return {"model_call_count": new_count}

# åˆ›å»ºä¸¤ä¸ª agent
llm = ChatOpenAI(model="kimi-k2")

# Store ç‰ˆæœ¬
agent_with_store = create_agent(
    model=llm,
    system_prompt="You are a helpful assistant.",
    store=InMemoryStore(),  # ä½¿ç”¨ store
    middleware=[count_with_store, increment_with_store]
)

# Checkpointer ç‰ˆæœ¬
agent_with_checkpointer = create_agent(
    model=llm,
    system_prompt="You are a helpful assistant.",
    checkpointer=InMemorySaver(),  # ä½¿ç”¨ checkpointer
    middleware=[count_with_checkpointer, increment_with_checkpointer]
)

def test_store_vs_checkpointer():
    print("ğŸ”¥ æµ‹è¯• Store ç‰ˆæœ¬ï¼ˆæ— çº¿ç¨‹IDï¼Œæ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰")
    print("=" * 50)

    state1 = {"messages": [], "model_call_count": 0, "user_id": "user1"}
    state1["messages"].append({"role": "user", "content": "ä½ å¥½"})
    result1 = agent_with_store.invoke(state1)

    state2 = result1
    state2["messages"].append({"role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ"})
    result2 = agent_with_store.invoke(state2)

    print(f"\nStoreç‰ˆæœ¬æœ€ç»ˆè®¡æ•°: {result2.get('model_call_count', 0)}")

    print("\nğŸ”¥ æµ‹è¯• Checkpointer ç‰ˆæœ¬ï¼ˆéœ€è¦çº¿ç¨‹IDï¼Œæ”¯æŒçŠ¶æ€æ¢å¤ï¼‰")
    print("=" * 50)

    config = {"configurable": {"thread_id": "conversation_1"}}

    # ç¬¬ä¸€æ¬¡è°ƒç”¨
    state1 = {"messages": [], "model_call_count": 0, "user_id": "user1"}
    state1["messages"].append({"role": "user", "content": "ä½ å¥½"})
    result1 = agent_with_checkpointer.invoke(state1, config=config)

    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåŸºäºçº¿ç¨‹IDè‡ªåŠ¨æ¢å¤çŠ¶æ€ï¼‰
    state2 = {"messages": [], "user_id": "user1"}  # æ³¨æ„ï¼šä¸éœ€è¦ä¼  model_call_count
    state2["messages"].append({"role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ"})
    result2 = agent_with_checkpointer.invoke(state2, config=config)

    print(f"\nCheckpointerç‰ˆæœ¬æœ€ç»ˆè®¡æ•°: {result2.get('model_call_count', 0)}")

if __name__ == "__main__":
    test_store_vs_checkpointer()